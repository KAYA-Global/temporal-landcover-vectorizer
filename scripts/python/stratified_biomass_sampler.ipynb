{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ulfboge/temporal-landcover-vectorizer/blob/main/scripts/python/stratified_biomass_sampler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Data Sampling and Integration\n",
    "\n",
    "This notebook:\n",
    "1. Processes multiple CSV files containing temporal spatial data\n",
    "2. Samples biomass data from 2021 in equal intervals\n",
    "3. Creates two unified long-format tables using the sampled pixel IDs:\n",
    "   - One without coordinates (original format)\n",
    "   - One with coordinates (x_coord, y_coord)\n",
    "4. Integrates corresponding NDVI and NDFI values for years 2015-2021\n",
    "\n",
    "## Setup\n",
    "First, let's mount Google Drive and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the working directory - adjust these paths according to your Google Drive structure\n",
    "csv_dir = \"/content/drive/MyDrive/earthengine/conversion/csv\"\n",
    "output_dir = \"/content/drive/MyDrive/earthengine/conversion/output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the years we want to process\n",
    "target_years = ['y2015', 'y2017', 'y2019', 'y2021']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Processing Functions\n",
    "Create function to process CSV files into long format, with option to include coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_to_long_format(file_path, include_coords=False):\n",
    "    # Extract the data type from filename (NDVI, NDFI, or biomass)\n",
    "    data_type = Path(file_path).stem.split('_')[0].upper()\n",
    "\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Identify year columns (starting with 'y') and filter for target years\n",
    "    year_cols = [col for col in df.columns if col in target_years]\n",
    "\n",
    "    # Define id_vars based on whether coordinates should be included\n",
    "    id_vars = ['pixel_id']\n",
    "    if include_coords:\n",
    "        id_vars.extend(['x_coord', 'y_coord'])\n",
    "\n",
    "    # Melt the dataframe to long format\n",
    "    long_df = pd.melt(\n",
    "        df,\n",
    "        id_vars=id_vars,\n",
    "        value_vars=year_cols,\n",
    "        var_name='year',\n",
    "        value_name=data_type\n",
    "    )\n",
    "\n",
    "    # Convert year format from 'y2013' to '2013'\n",
    "    long_df['year'] = long_df['year'].str.replace('y', '')\n",
    "\n",
    "    return long_df\n",
    "\n",
    "def get_2021_biomass_data(file_path):\n",
    "    \"\"\"Extract 2021 biomass data for initial sampling\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df[['pixel_id', 'y2021']].rename(columns={'y2021': 'BIOMASS'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from 2021 Biomass Data\n",
    "Sample 100 points from 2021 biomass data with equal distribution across intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find biomass file\n",
    "biomass_file = None\n",
    "for file in os.listdir(csv_dir):\n",
    "    if file.startswith('biomass') and file.endswith('.csv'):\n",
    "        biomass_file = os.path.join(csv_dir, file)\n",
    "        break\n",
    "\n",
    "if not biomass_file:\n",
    "    raise FileNotFoundError(\"No biomass CSV file found\")\n",
    "\n",
    "# Get 2021 biomass data for sampling\n",
    "biomass_2021_df = get_2021_biomass_data(biomass_file)\n",
    "\n",
    "# Define intervals and sample biomass data\n",
    "intervals = [(5, 34), (35, 64), (65, 94), (95, 124), (125, 150)]\n",
    "samples_per_interval = 20  # 20 samples per interval = 100 total\n",
    "sampled_pixels = []\n",
    "\n",
    "for start, end in intervals:\n",
    "    # Filter data within interval\n",
    "    interval_data = biomass_2021_df[\n",
    "        (biomass_2021_df['BIOMASS'] >= start) &\n",
    "        (biomass_2021_df['BIOMASS'] <= end)\n",
    "    ]\n",
    "\n",
    "    # Sample from this interval\n",
    "    if len(interval_data) >= samples_per_interval:\n",
    "        sampled = interval_data.sample(n=samples_per_interval, random_state=42)\n",
    "    else:\n",
    "        sampled = interval_data  # Take all available if less than needed\n",
    "        print(f\"Warning: Only {len(interval_data)} samples available for interval {start}-{end}\")\n",
    "\n",
    "    sampled_pixels.extend(sampled['pixel_id'].unique())\n",
    "\n",
    "print(f\"Total unique pixels sampled from 2021 data: {len(set(sampled_pixels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All CSV Files\n",
    "Process all CSV files and create two versions of each dataset - with and without coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all CSV files - create two versions\n",
    "dfs = {'with_coords': {}, 'without_coords': {}}\n",
    "for file in os.listdir(csv_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_dir, file)\n",
    "        data_type = Path(file).stem.split('_')[0].upper()\n",
    "        dfs['with_coords'][data_type] = process_csv_to_long_format(file_path, include_coords=True)\n",
    "        dfs['without_coords'][data_type] = process_csv_to_long_format(file_path, include_coords=False)\n",
    "        print(f\"Processed {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Merge Data\n",
    "Create filtered datasets using the sampled pixel IDs and merge them to create final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two versions of the final result\n",
    "filtered_dfs = {\n",
    "    'with_coords': {},\n",
    "    'without_coords': {}\n",
    "}\n",
    "\n",
    "# Filter each dataset for sampled pixels\n",
    "for version in ['with_coords', 'without_coords']:\n",
    "    for data_type, df in dfs[version].items():\n",
    "        filtered_dfs[version][data_type] = df[df['pixel_id'].isin(sampled_pixels)]\n",
    "\n",
    "# Create and save both versions\n",
    "results = {}\n",
    "\n",
    "# Version without coordinates (original)\n",
    "result_without_coords = filtered_dfs['without_coords']['BIOMASS'][['pixel_id', 'year', 'BIOMASS']]\n",
    "for data_type in ['NDVI', 'NDFI']:\n",
    "    if data_type in filtered_dfs['without_coords']:\n",
    "        result_without_coords = result_without_coords.merge(\n",
    "            filtered_dfs['without_coords'][data_type][['pixel_id', 'year', data_type]],\n",
    "            on=['pixel_id', 'year'],\n",
    "            how='left'\n",
    "        )\n",
    "results['without_coords'] = result_without_coords.sort_values(['pixel_id', 'year'])\n",
    "\n",
    "# Version with coordinates\n",
    "result_with_coords = filtered_dfs['with_coords']['BIOMASS'][['pixel_id', 'x_coord', 'y_coord', 'year', 'BIOMASS']]\n",
    "for data_type in ['NDVI', 'NDFI']:\n",
    "    if data_type in filtered_dfs['with_coords']:\n",
    "        result_with_coords = result_with_coords.merge(\n",
    "            filtered_dfs['with_coords'][data_type][['pixel_id', 'year', data_type]],\n",
    "            on=['pixel_id', 'year'],\n",
    "            how='left'\n",
    "        )\n",
    "results['with_coords'] = result_with_coords.sort_values(['pixel_id', 'year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results and Display Summary\n",
    "Save both versions of the results and display summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both versions\n",
    "output_path_without_coords = os.path.join(output_dir, 'sampled_data.csv')\n",
    "output_path_with_coords = os.path.join(output_dir, 'sampled_data_with_coords.csv')\n",
    "\n",
    "results['without_coords'].to_csv(output_path_without_coords, index=False)\n",
    "results['with_coords'].to_csv(output_path_with_coords, index=False)\n",
    "\n",
    "print(f\"\\nResults saved to:\")\n",
    "print(f\"1. Without coordinates: {output_path_without_coords}\")\n",
    "print(f\"2. With coordinates: {output_path_with_coords}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary of sampled data:\")\n",
    "print(f\"Total unique pixels: {results['with_coords']['pixel_id'].nunique()}\")\n",
    "print(f\"Years covered: {sorted(results['with_coords']['year'].unique())}\")\n",
    "print(\"\\nValue ranges by year:\")\n",
    "for year in sorted(results['with_coords']['year'].unique()):\n",
    "    print(f\"\\nYear {year}:\")\n",
    "    year_data = results['with_coords'][results['with_coords']['year'] == year]\n",
    "    for column in ['BIOMASS', 'NDVI', 'NDFI']:\n",
    "        if column in year_data.columns:\n",
    "            print(f\"  {column}:\")\n",
    "            print(f\"    Min: {year_data[column].min():.2f}\")\n",
    "            print(f\"    Max: {year_data[column].max():.2f}\")\n",
    "            print(f\"    Mean: {year_data[column].mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
