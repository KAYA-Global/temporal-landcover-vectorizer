{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ulfboge/temporal-landcover-vectorizer/blob/main/scripts/python/stratified_biomass_sampler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zfCGlb_agEh"
      },
      "source": [
        "# Spatial Data Sampling and Integration\n",
        "\n",
        "This notebook:\n",
        " 1. Processes multiple CSV files containing temporal spatial data\n",
        " 2. Samples biomass data from 2021 in equal intervals based on coordinates\n",
        " 3. Creates a unified long-format table with coordinates (x_coord, y_coord)\n",
        " 4. Integrates corresponding NDVI and NDFI values for years 2015-2021\n",
        "\n",
        "## Setup\n",
        " First, let's mount Google Drive and import required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwlpApSjagEj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AbByEl2agEj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set the working directory - adjust these paths according to your Google Drive structure\n",
        "csv_dir = \"/content/drive/MyDrive/earthengine/conversion/csv\"\n",
        "output_dir = \"/content/drive/MyDrive/earthengine/conversion/output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Define the years we want to process\n",
        "target_years = ['y2015', 'y2017', 'y2019', 'y2021']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykNFpofragEk"
      },
      "source": [
        "## Define Processing Functions\n",
        "Create function to process CSV files into long format, with option to include coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mrZCwZEagEk"
      },
      "outputs": [],
      "source": [
        "def process_csv_to_long_format(file_path):\n",
        "    # Extract the data type from filename (NDVI, NDFI, or biomass)\n",
        "    data_type = Path(file_path).stem.split('_')[0].upper()\n",
        "\n",
        "    # Read CSV\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Identify year columns (starting with 'y') and filter for target years\n",
        "    year_cols = [col for col in df.columns if col in target_years]\n",
        "\n",
        "    # Always include coordinates\n",
        "    id_vars = ['x_coord', 'y_coord']\n",
        "\n",
        "    # Melt the dataframe to long format\n",
        "    long_df = pd.melt(\n",
        "        df,\n",
        "        id_vars=id_vars,\n",
        "        value_vars=year_cols,\n",
        "        var_name='year',\n",
        "        value_name=data_type\n",
        "    )\n",
        "\n",
        "    # Convert year format from 'y2013' to '2013'\n",
        "    long_df['year'] = long_df['year'].str.replace('y', '')\n",
        "\n",
        "    return long_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO-ZmfJIagEk"
      },
      "source": [
        "# Function to get 2021 biomass data for initial sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl61lrYmagEk"
      },
      "outputs": [],
      "source": [
        "def get_2021_biomass_data(file_path):\n",
        "    \"\"\"Extract 2021 biomass data for initial sampling\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df[['x_coord', 'y_coord', 'y2021']].rename(columns={'y2021': 'BIOMASS'})\n",
        "\n",
        "## Sample from 2021 Biomass Data\n",
        "# Sample 100 points from 2021 biomass data with equal distribution across intervals based on coordinates.\n",
        "\n",
        "# Find biomass file\n",
        "biomass_file = None\n",
        "for file in os.listdir(csv_dir):\n",
        "    if file.startswith('biomass') and file.endswith('.csv'):\n",
        "        biomass_file = os.path.join(csv_dir, file)\n",
        "        break\n",
        "\n",
        "if not biomass_file:\n",
        "    raise FileNotFoundError(\"No biomass CSV file found\")\n",
        "\n",
        "# Get 2021 biomass data for sampling\n",
        "biomass_2021_df = get_2021_biomass_data(biomass_file)\n",
        "\n",
        "# Define intervals and sample biomass data\n",
        "intervals = [(5, 34), (35, 64), (65, 94), (95, 124), (125, 150)]\n",
        "samples_per_interval = 40  # 20 samples per interval = 100 total\n",
        "sampled_coords = [] # Store sampled coordinates as tuples\n",
        "\n",
        "\n",
        "for start, end in intervals:\n",
        "    # Filter data within interval\n",
        "    interval_data = biomass_2021_df[\n",
        "        (biomass_2021_df['BIOMASS'] >= start) &\n",
        "        (biomass_2021_df['BIOMASS'] <= end)\n",
        "    ]\n",
        "\n",
        "    # Sample from this interval\n",
        "    if len(interval_data) >= samples_per_interval:\n",
        "        sampled = interval_data.sample(n=samples_per_interval, random_state=42)\n",
        "    else:\n",
        "        sampled = interval_data  # Take all available if less than needed\n",
        "        print(f\"Warning: Only {len(interval_data)} samples available for interval {start}-{end}\")\n",
        "    # Add the selected coordinate tuples to the list\n",
        "    for index in sampled.index:\n",
        "        sampled_coords.append(tuple(sampled.loc[index, ['x_coord', 'y_coord']]))\n",
        "\n",
        "print(f\"Total coordinate pairs sampled from 2021 data: {len(set(sampled_coords))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGTRS4LOagEl"
      },
      "source": [
        "## Process All CSV Files\n",
        "Process all CSV files and create datasets with coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss9xRw9sagEl"
      },
      "outputs": [],
      "source": [
        "dfs = {'with_coords': {}}\n",
        "for file in os.listdir(csv_dir):\n",
        "    if file.endswith('.csv'):\n",
        "        file_path = os.path.join(csv_dir, file)\n",
        "        data_type = Path(file).stem.split('_')[0].upper()\n",
        "        dfs['with_coords'][data_type] = process_csv_to_long_format(file_path)\n",
        "        print(f\"Processed {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9J5MfOfagEl"
      },
      "source": [
        "## Filter and Merge Data\n",
        "Create filtered datasets using the sampled coordinates and merge them to create final results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7yOTgM3agEl"
      },
      "outputs": [],
      "source": [
        "# Filter each dataset for sampled pixels\n",
        "filtered_dfs = {}  # No need for nested dictionary\n",
        "\n",
        "for data_type, df in dfs['with_coords'].items():\n",
        "    # Filter by coordinate pairs\n",
        "    filtered_dfs[data_type] = df[\n",
        "        df[['x_coord', 'y_coord']].apply(tuple, axis=1).isin(sampled_coords)\n",
        "    ]\n",
        "\n",
        "# Create the final result with coordinates\n",
        "result_with_coords = filtered_dfs['BIOMASS'][['x_coord', 'y_coord', 'year', 'BIOMASS']]\n",
        "for data_type in ['NDVI', 'NDFI']:\n",
        "    if data_type in filtered_dfs:\n",
        "        result_with_coords = result_with_coords.merge(\n",
        "            filtered_dfs[data_type][['x_coord', 'y_coord', 'year', data_type]],\n",
        "            on=['x_coord', 'y_coord', 'year'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "results = {'with_coords': result_with_coords.sort_values(['x_coord', 'y_coord', 'year'])}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUFQTn6ZagEl"
      },
      "source": [
        "## Save Results and Display Summary\n",
        "Save the results and display summary statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpvs9zOHagEl"
      },
      "outputs": [],
      "source": [
        "# Save results\n",
        "output_path = os.path.join(output_dir, 'sampled_data_with_coords.csv')  # Simplified name\n",
        "\n",
        "results['with_coords'].to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\nResults saved to: {output_path}\")\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nSummary of sampled data:\")\n",
        "print(f\"Total coordinate pairs: {results['with_coords'][['x_coord', 'y_coord']].apply(tuple, axis=1).nunique()}\")\n",
        "print(f\"Years covered: {sorted(results['with_coords']['year'].unique())}\")\n",
        "print(\"\\nValue ranges by year:\")\n",
        "for year in sorted(results['with_coords']['year'].unique()):\n",
        "    print(f\"\\nYear {year}:\")\n",
        "    year_data = results['with_coords'][results['with_coords']['year'] == year]\n",
        "    for column in ['BIOMASS', 'NDVI', 'NDFI']:\n",
        "        if column in year_data.columns:\n",
        "            print(f\"  {column}:\")\n",
        "            print(f\"    Min: {year_data[column].min():.2f}\")\n",
        "            print(f\"    Max: {year_data[column].max():.2f}\")\n",
        "            print(f\"    Mean: {year_data[column].mean():.2f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
