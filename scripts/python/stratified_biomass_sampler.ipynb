{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ulfboge/temporal-landcover-vectorizer/blob/main/scripts/python/stratified_biomass_sampler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjN37WTuU13_"
      },
      "source": [
        "# Spatial Data Sampling and Integration\n",
        "\n",
        "This notebook:\n",
        "1. Processes multiple CSV files containing temporal spatial data\n",
        "2. Creates a unified long-format table\n",
        "3. Samples biomass data from 2021 in equal intervals\n",
        "4. Integrates corresponding NDVI and NDFI values for all years using the same pixel_ids\n",
        "\n",
        "## Setup\n",
        "First, let's mount Google Drive and import required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vReaNc6qU14B"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNE5aXqNU14C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set the working directory\n",
        "csv_dir = \"/content/drive/MyDrive/earthengine/conversion/csv\"\n",
        "output_dir = \"/content/drive/MyDrive/earthengine/conversion/output\"\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECNFWOVLU14C"
      },
      "source": [
        "## Process CSV Files\n",
        "Read and transform CSV files into long format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-q94FU2U14C"
      },
      "outputs": [],
      "source": [
        "def process_csv_to_long_format(file_path):\n",
        "    # Extract the data type from filename (NDVI, NDFI, or biomass)\n",
        "    data_type = Path(file_path).stem.split('_')[0].upper()\n",
        "\n",
        "    # Read CSV\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Define target years based on data type\n",
        "    target_years = ['y2015', 'y2017', 'y2019', 'y2021']\n",
        "\n",
        "    # Filter only the columns we want\n",
        "    columns_to_keep = ['pixel_id'] + target_years\n",
        "    available_columns = [col for col in columns_to_keep if col in df.columns]\n",
        "    df = df[available_columns]\n",
        "\n",
        "    # Melt the dataframe to long format\n",
        "    long_df = pd.melt(\n",
        "        df,\n",
        "        id_vars=['pixel_id'],\n",
        "        value_vars=[col for col in target_years if col in df.columns],\n",
        "        var_name='year',\n",
        "        value_name=data_type\n",
        "    )\n",
        "\n",
        "    # Convert year format from 'y2013' to '2013'\n",
        "    long_df['year'] = long_df['year'].str.replace('y', '')\n",
        "\n",
        "    return long_df\n",
        "\n",
        "# Process all CSV files\n",
        "dfs = {}\n",
        "for file in os.listdir(csv_dir):\n",
        "    if file.endswith('.csv'):\n",
        "        file_path = os.path.join(csv_dir, file)\n",
        "        data_type = Path(file).stem.split('_')[0].upper()\n",
        "        dfs[data_type] = process_csv_to_long_format(file_path)\n",
        "        print(f\"Processed {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FlVYt-SU14D"
      },
      "source": [
        "## Sample Biomass Data from 2021\n",
        "Sample 100 points from biomass data with equal distribution across intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKpOTXWAU14D"
      },
      "outputs": [],
      "source": [
        "# Define intervals\n",
        "intervals = [(5, 34), (35, 64), (65, 94), (95, 124), (125, 150)]\n",
        "samples_per_interval = 20  # 20 samples per interval = 100 total\n",
        "\n",
        "# Filter biomass data for 2021\n",
        "biomass_df = dfs['BIOMASS']\n",
        "biomass_2021 = biomass_df[biomass_df['year'] == '2021']\n",
        "sampled_pixels = []\n",
        "\n",
        "for start, end in intervals:\n",
        "    # Filter data within interval for 2021\n",
        "    interval_data = biomass_2021[\n",
        "        (biomass_2021['BIOMASS'] >= start) &\n",
        "        (biomass_2021['BIOMASS'] <= end)\n",
        "    ]\n",
        "\n",
        "    # Sample from this interval\n",
        "    if len(interval_data) >= samples_per_interval:\n",
        "        sampled = interval_data.sample(n=samples_per_interval, random_state=42)\n",
        "    else:\n",
        "        sampled = interval_data  # Take all available if less than needed\n",
        "        print(f\"Warning: Only {len(interval_data)} samples available for interval {start}-{end}\")\n",
        "\n",
        "    sampled_pixels.extend(sampled['pixel_id'].unique())\n",
        "\n",
        "print(f\"Total unique pixels sampled from 2021: {len(set(sampled_pixels))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk3BgHp_U14D"
      },
      "source": [
        "## Integrate Data\n",
        "Combine data from all sources using sampled pixel IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6KuwYPpU14D"
      },
      "outputs": [],
      "source": [
        "# Filter each dataset for sampled pixels\n",
        "filtered_dfs = {}\n",
        "for data_type, df in dfs.items():\n",
        "    filtered_dfs[data_type] = df[df['pixel_id'].isin(sampled_pixels)]\n",
        "\n",
        "# Merge all datasets\n",
        "result = filtered_dfs['BIOMASS'][['pixel_id', 'year', 'BIOMASS']]\n",
        "for data_type in ['NDVI', 'NDFI']:\n",
        "    if data_type in filtered_dfs:\n",
        "        result = result.merge(\n",
        "            filtered_dfs[data_type][['pixel_id', 'year', data_type]],\n",
        "            on=['pixel_id', 'year'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "# Sort the results\n",
        "result = result.sort_values(['pixel_id', 'year'])\n",
        "\n",
        "# Save the results\n",
        "output_path = os.path.join(output_dir, 'sampled_data.csv')\n",
        "result.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Results saved to: {output_path}\")\n",
        "print(\"\\nFirst few rows of the result:\")\n",
        "display(result.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e393LbFU14D"
      },
      "source": [
        "## Summary Statistics\n",
        "Display summary of the sampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1QPY66LU14E"
      },
      "outputs": [],
      "source": [
        "print(\"Summary of sampled data:\")\n",
        "print(f\"Total unique pixels: {result['pixel_id'].nunique()}\")\n",
        "print(f\"Years covered: {sorted(result['year'].unique())}\")\n",
        "print(\"\\nValue ranges:\")\n",
        "\n",
        "# Add year-specific statistics\n",
        "for year in sorted(result['year'].unique()):\n",
        "    year_data = result[result['year'] == year]\n",
        "    print(f\"\\nYear {year}:\")\n",
        "    for column in ['BIOMASS', 'NDVI', 'NDFI']:\n",
        "        if column in result.columns:\n",
        "            print(f\"{column}:\")\n",
        "            print(f\"  Min: {year_data[column].min():.2f}\")\n",
        "            print(f\"  Max: {year_data[column].max():.2f}\")\n",
        "            print(f\"  Mean: {year_data[column].mean():.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}